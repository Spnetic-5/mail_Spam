{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "SpamDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sOgIC9vgIyx5"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBGPH0VtIyxr"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB9EGf7fIyxw"
      },
      "source": [
        "data = pd.read_csv(\"spam.csv\",encoding = \"'latin'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWG8DrwIIyxy"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDo-2eunIyx2"
      },
      "source": [
        "data[\"text\"] = data.v2\n",
        "data[\"spam\"] = data.v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOgIC9vgIyx5"
      },
      "source": [
        "# Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKqqW3y0Iyx5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "emails_train, emails_test, target_train, target_test = train_test_split(data.text,data.spam,test_size = 0.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhaVY-xSIyx8"
      },
      "source": [
        "data.info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_7DJm1kIyx-"
      },
      "source": [
        "emails_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5cfcZhuIyyF"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDFx5cIIyyF"
      },
      "source": [
        "def remove_hyperlink(word):\n",
        "    return  re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "    return result\n",
        "\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', '', word)\n",
        "    return result\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "    return result\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "    return result\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n','')\n",
        "\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink,\n",
        "                      replace_newline,\n",
        "                      to_lower,\n",
        "                      remove_number,\n",
        "                      remove_punctuation,remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence\n",
        "\n",
        "x_train = [clean_up_pipeline(o) for o in emails_train]\n",
        "x_test = [clean_up_pipeline(o) for o in emails_test]\n",
        "\n",
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6m0KYITIyyI"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train_y = le.fit_transform(target_train.values)\n",
        "test_y = le.transform(target_test.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8nTaChYIyyK"
      },
      "source": [
        "train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQnP3LyeIyyN"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRfyW-pHIyyN"
      },
      "source": [
        "## some config values \n",
        "embed_size = 100 # how big is each word vector\n",
        "max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 2000 # max number of words in a question to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pU9YkDwIyyQ"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_feature)\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_features = np.array(tokenizer.texts_to_sequences(x_train))\n",
        "x_test_features = np.array(tokenizer.texts_to_sequences(x_test))\n",
        "\n",
        "x_train_features[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYBAvFS-IyyT"
      },
      "source": [
        "# Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9qrzRgtIyyU"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "x_train_features = pad_sequences(x_train_features,maxlen=max_len)\n",
        "x_test_features = pad_sequences(x_test_features,maxlen=max_len)\n",
        "x_train_features[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNJGhByLIyyZ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ppV2YhHIyyZ"
      },
      "source": [
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nXG9MfEKWzx"
      },
      "source": [
        "# create the model\n",
        "import tensorflow as tf\n",
        "embedding_vecor_length = 32\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Embedding(max_feature, embedding_vecor_length, input_length=max_len))\n",
        "model.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_oFHNH6Iyye"
      },
      "source": [
        "history = model.fit(x_train_features, train_y, batch_size=512, epochs=20, validation_data=(x_test_features, test_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpAgrydLNQ3G"
      },
      "source": [
        "from  matplotlib import pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzGF_hA4Jg3m"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47vMs8s-maLQ"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt=''); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');\n",
        "ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkJQ-yURJg3n"
      },
      "source": [
        "y_predict  = [1 if o>0.5 else 0 for o in model.predict(x_test_features)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB21196PJg3o"
      },
      "source": [
        "cf_matrix =confusion_matrix(test_y,y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kberNYWIJg3r"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(test_y,y_predict).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kshhXwFAJg3s"
      },
      "source": [
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(test_y, y_predict)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(test_y, y_predict)))\n",
        "print(\"F1 Score: {:.2f}%\".format(100 * f1_score(test_y,y_predict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8pacWZkJg3v"
      },
      "source": [
        "f1_score(test_y,y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}